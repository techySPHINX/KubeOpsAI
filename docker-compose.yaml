version: '3.8'

services:
  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - C:\Users\KIIT\desktop\minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  nessie:
    image: projectnessie/nessie
    ports:
      - "19120:19120"
    environment:
      NESSIE_VERSION_STORE_TYPE: IN_MEMORY
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/q/health"]
      interval: 30s
      timeout: 20s
      retries: 3

  spark-iceberg:
    image: tabulario/spark-iceberg:latest
    ports:
      - "8080:8080" # Spark UI
      - "8081:8081" # Spark History Server
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin
      AWS_REGION: us-east-1 # Required by Spark S3 connector
      MINIO_URL: http://minio:9000
      NESSIE_URL: http://nessie:19120/api/v1
      SPARK_MASTER_HOST: spark-iceberg
    volumes:
      - C:\Users\KIIT\desktop\spark_data:/opt/spark/work-dir
    command: bash -c "/opt/spark/bin/spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.0,org.projectnessie:nessie-spark-extensions-3.4_2.12:0.76.0 --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions --conf spark.sql.catalog.nessie=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.nessie.uri=http://nessie:19120/api/v1 --conf spark.sql.catalog.nessie.ref=main --conf spark.sql.catalog.nessie.authentication.type=NONE --conf spark.sql.catalog.nessie.warehouse=s3a://warehouse --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minioadmin --conf spark.hadoop.fs.s3a.secret.key=minioadmin --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    depends_on:
      - minio
      - nessie

